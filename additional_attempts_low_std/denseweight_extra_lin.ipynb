{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: denseweight in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from denseweight) (2.2.2)\n",
      "Requirement already satisfied: KDEpy in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from denseweight) (1.1.11)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from denseweight) (1.6.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.0.1 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from KDEpy->denseweight) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from scikit-learn->denseweight) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from scikit-learn->denseweight) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000000,)\n",
      "[2.09604394]\n",
      "Train Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800000 entries, 0 to 799999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   Series1  800000 non-null  float64\n",
      " 1   Series2  800000 non-null  float64\n",
      " 2   Series3  800000 non-null  float64\n",
      " 3   Label    800000 non-null  float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 24.4 MB\n",
      "None\n",
      "\n",
      "Test Data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   Series1  200000 non-null  float64\n",
      " 1   Series2  200000 non-null  float64\n",
      " 2   Series3  200000 non-null  float64\n",
      " 3   Label    200000 non-null  float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 6.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas -q\n",
    "!pip install denseweight\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from denseweight import DenseWeight\n",
    "\n",
    "\n",
    "# File paths\n",
    "train_file = \"./extra_lin_around_mean_train_low_std_range_very_low_std_standardized_log1p.csv\"\n",
    "test_file = \"./extra_lin_around_mean_test_low_std_range_very_low_std_standardized_log1p.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "dw = DenseWeight(alpha=1.0)\n",
    "print(np.random.normal(size=1000).shape)\n",
    "print(pd.concat([train_df[\"Label\"], test_df[\"Label\"]]).to_numpy().shape)\n",
    "weights = dw.fit(pd.concat([train_df[\"Label\"], test_df[\"Label\"]]).to_numpy())\n",
    "train_weights = dw(train_df[\"Label\"].to_numpy())\n",
    "test_weights = dw(test_df[\"Label\"].to_numpy())\n",
    "print(dw([6]))\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"Train Data:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series1</th>\n",
       "      <th>Series2</th>\n",
       "      <th>Series3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>800000.000000</td>\n",
       "      <td>800000.000000</td>\n",
       "      <td>800000.000000</td>\n",
       "      <td>8.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000727</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>3.165487e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>5.774442e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.311488</td>\n",
       "      <td>-1.714816</td>\n",
       "      <td>-1.714816</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.457694</td>\n",
       "      <td>-0.848747</td>\n",
       "      <td>-0.848747</td>\n",
       "      <td>4.289651e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.017321</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>9.727914e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.883390</td>\n",
       "      <td>0.883390</td>\n",
       "      <td>4.120072e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.311415</td>\n",
       "      <td>1.714816</td>\n",
       "      <td>1.714816</td>\n",
       "      <td>5.991320e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Series1        Series2        Series3         Label\n",
       "count  800000.000000  800000.000000  800000.000000  8.000000e+05\n",
       "mean       -0.000727       0.000195       0.000584  3.165487e-01\n",
       "std         0.999889       0.999988       0.999805  5.774442e-01\n",
       "min        -2.311488      -1.714816      -1.714816  0.000000e+00\n",
       "25%        -0.457694      -0.848747      -0.848747  4.289651e-09\n",
       "50%        -0.000698      -0.017321       0.017321  9.727914e-02\n",
       "75%         0.456647       0.883390       0.883390  4.120072e-01\n",
       "max         2.311415       1.714816       1.714816  5.991320e+00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sinabs in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: pbr in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (6.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (2.2.2)\n",
      "Requirement already satisfied: torch>=1.8 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (2.4.1)\n",
      "Requirement already satisfied: nir in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (1.0.4)\n",
      "Requirement already satisfied: nirtorch in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (1.0)\n",
      "Requirement already satisfied: samna>=0.33 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sinabs) (0.41.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from torch>=1.8->sinabs) (2024.12.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from nir->sinabs) (3.12.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from jinja2->torch>=1.8->sinabs) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lutz\\bachelorarbeit2\\snn\\.venv\\lib\\site-packages (from sympy->torch>=1.8->sinabs) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install norse and pytorch\n",
    "!pip install sinabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.inputval = dataframe[\"Series1\"]\n",
    "        self.mean = dataframe[\"Series2\"]\n",
    "        self.std = dataframe[\"Series3\"]\n",
    "        self.labels = dataframe[\"Label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputval)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputval = self.inputval.iloc[idx]\n",
    "        mean = self.mean.iloc[idx]\n",
    "        std = self.std.iloc[idx]\n",
    "        labels = self.labels.iloc[idx]\n",
    "        return inputval, mean, std, labels\n",
    "    \n",
    "# Create the datasets\n",
    "train_dataset = DataFrameDataset(train_df)\n",
    "test_dataset = DataFrameDataset(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "train_sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input data\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64) #, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 132865\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "TRAINING_PREFIX = \"dense_weights_extra_lin\"\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Linear(3, 256),  # Input layer: 3 features (mu, sigma, x)\n",
    "    nn.ReLU(),\n",
    "    #nn.Dropout(0.2),\n",
    "    nn.Linear(256, 256),  # first hidden layer\n",
    "    nn.ReLU(),\n",
    "    #nn.Dropout(0.2),\n",
    "    nn.Linear(256, 256),  # second hidden layer\n",
    "    nn.ReLU(),\n",
    "    #nn.Dropout(0.2),\n",
    "    nn.Linear(256, 1)    # Output layer: single value for f(x; mu, sigma)\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "#ann.load_state_dict(torch.load(f\"results/{TRAINING_PREFIX}/ann_epoch_300.pth\"))\n",
    "\n",
    "ann.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in ann.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (spiking_model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "    tensor(1.), min_v_mem=Parameter containing:\n",
       "    tensor(-1.), batch_size=-1, num_timesteps=100)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "    tensor(1.), min_v_mem=Parameter containing:\n",
       "    tensor(-1.), batch_size=-1, num_timesteps=100)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "    tensor(1.), min_v_mem=Parameter containing:\n",
       "    tensor(-1.), batch_size=-1, num_timesteps=100)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (spike_output): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "    tensor(1.), min_v_mem=Parameter containing:\n",
       "    tensor(-1.), batch_size=-1, num_timesteps=100)\n",
       "  )\n",
       "  (analog_model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "num_time_steps_per_sample = 100\n",
    "\n",
    "sinabs_model = from_model(ann, input_shape=(3,), add_spiking_output=True, synops=False, num_timesteps=num_time_steps_per_sample)\n",
    "sinabs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torcheval -q\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "\n",
    "def create_loss_plot():\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train Loss', line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Eval Loss', line=dict(color='orange')))\n",
    "\n",
    "    # Configure layout\n",
    "    fig.update_layout(title='Training and Evaluation Losses',\n",
    "                    xaxis_title='Epoch',\n",
    "                    yaxis_title='Loss',\n",
    "                    template='plotly_dark')\n",
    "\n",
    "    # Display the figure widget\n",
    "    display(fig)\n",
    "    return fig\n",
    "\n",
    "def update_loss_plot(fig, train_loss, eval_loss, from_epoch=0):\n",
    "    if from_epoch != 0:\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = list(range(from_epoch, len(train_loss)))\n",
    "            fig.data[0].y = train_loss[from_epoch:]\n",
    "            fig.data[1].x = list(range(from_epoch, len(eval_loss)))\n",
    "            fig.data[1].y = eval_loss[from_epoch:]\n",
    "    elif len(train_loss) < 30:\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = list(range(len(train_loss)))\n",
    "            fig.data[0].y = train_loss\n",
    "            fig.data[1].x = list(range(len(eval_loss)))\n",
    "            fig.data[1].y = eval_loss\n",
    "    else:\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x = list(range(len(train_loss) - 30, len(train_loss)))\n",
    "            fig.data[0].y = train_loss[-30:]\n",
    "            fig.data[1].x = list(range(len(eval_loss) - 30, len(eval_loss)))\n",
    "            fig.data[1].y = eval_loss[-30:]        \n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "def gaussian_probability(x, y, z):\n",
    "    return (1 / (np.sqrt(2 * np.pi) * z)) * np.exp(-((x - y) ** 2) / (2 * z ** 2))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_epoch(loader, model, optimizer, device, n_epochs: int, current_epoch: int, train: bool = False,):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    epoch_loss = 0\n",
    "    metric = R2Score(device=device)\n",
    "    for inputvals, means, stds, labels in tqdm(loader, desc=f\"{'Epoch' if train else 'Eval Epoch'} {current_epoch+1}/{n_epochs}\"):\n",
    "        inputs = torch.stack((inputvals, means, stds), dim=1).float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        metric.update(outputs.squeeze(), labels)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            loss = F.mse_loss(outputs.squeeze(), labels.float())\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss, metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cda2aaf4e648cd80b3cb90790156a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'color': 'blue'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Train Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ab202a82-b2c7-4250-9907-b5b8eaddd380',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'line': {'color': 'orange'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Eval Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '27706776-45d2-4afd-a42d-eb61a34e87a0',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training and Evaluation Losses'},\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a905e41b40f41eb9ef4036e41ce73d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 990/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524379f9fd2e4101808cdbcea27a43f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 990/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 990, Train Loss: 0.015761945167085067, Diff: 0.00020990868780739955, Eval Loss: 0.007628089150059968, Diff Eval: -0.004385946914472152, Train R2 Score: 0.9693068265914917, Eval R2 Score: 0.9771743416786194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c843e084274963a4ef80d3a0a9d791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 991/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d0850101374ddcbf853fafa7506257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 991/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 991, Train Loss: 0.015469156312263222, Diff: -0.0002927888548218457, Eval Loss: 0.009367491512391716, Diff Eval: 0.001739402362331748, Train R2 Score: 0.9696730375289917, Eval R2 Score: 0.971969485282898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70400b1badd4ce3833389f3672fe403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 992/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c14d081b1974453901159fe7a4d5c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 992/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 992, Train Loss: 0.015971138331556722, Diff: 0.0005019820192935006, Eval Loss: 0.0074334078469692035, Diff Eval: -0.0019340836654225127, Train R2 Score: 0.9691053032875061, Eval R2 Score: 0.9777568578720093\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aa8bdddb1845a38502020eb6008946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 993/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9352d331644244dc85834285815b9f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 993/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 993, Train Loss: 0.01590518402949885, Diff: -6.595430205787178e-05, Eval Loss: 0.009860993370553479, Diff Eval: 0.0024275855235842752, Train R2 Score: 0.9688858985900879, Eval R2 Score: 0.9704927206039429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f8a20a2d16407797b6c1a982e49d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 994/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d3fe8721f34605b85f45664cd7bfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 994/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 994, Train Loss: 0.01574403538099341, Diff: -0.00016114864850544017, Eval Loss: 0.008357806628737599, Diff Eval: -0.0015031867418158797, Train R2 Score: 0.9690234065055847, Eval R2 Score: 0.9749907851219177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d354d046c749c1947bed3446f9ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 995/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abed0496ec394840821eb452af45f6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 995/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 995, Train Loss: 0.015598183585386905, Diff: -0.00014585179560650495, Eval Loss: 0.008523359317235881, Diff Eval: 0.00016555268849828206, Train R2 Score: 0.9698210954666138, Eval R2 Score: 0.9744953513145447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d746d83c7a847dfb15ede8bed580c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 996/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c28cb7529c04327858be402c39541f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 996/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 996, Train Loss: 0.016290359424195267, Diff: 0.000692175838808361, Eval Loss: 0.008860329206502066, Diff Eval: 0.00033696988926618446, Train R2 Score: 0.967818558216095, Eval R2 Score: 0.9734870195388794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14558c141b547108905d3a5725da1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 997/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659c16ce748f415690726ada4a43b805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 997/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997, Train Loss: 0.015880844201780712, Diff: -0.0004095152224145547, Eval Loss: 0.006523808688269928, Diff Eval: -0.0023365205182321377, Train R2 Score: 0.9689919352531433, Eval R2 Score: 0.9804786443710327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0223f5c0ae2460ca5d26be3d92b8570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 998/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a227902614b4556ac8dece6edb5f1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 998/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 998, Train Loss: 0.015723675603818373, Diff: -0.00015716859796233856, Eval Loss: 0.010374273518454283, Diff Eval: 0.003850464830184355, Train R2 Score: 0.9693170785903931, Eval R2 Score: 0.9689567685127258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e470c9689e37417a8978523ce305419c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 999/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5994a57fed064153a4fbac66af3efd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 999/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999, Train Loss: 0.016313121266592717, Diff: 0.0005894456627743433, Eval Loss: 0.007837506301517133, Diff Eval: -0.0025367672169371493, Train R2 Score: 0.9682667851448059, Eval R2 Score: 0.9765476584434509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17af81d8287942c88897b89ca328cc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1000/1000:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fb18e17d9541178d87dea6b8ad000c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Epoch 1000/1000:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Train Loss: 0.016453526719111978, Diff: 0.00014040545251926134, Eval Loss: 0.009325923200689722, Diff Eval: 0.0014884168991725882, Train R2 Score: 0.9676513671875, Eval R2 Score: 0.9720938205718994\n"
     ]
    }
   ],
   "source": [
    "ann.train()\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "LR = 0.01\n",
    "SAVE_EVERY = 50\n",
    "\n",
    "# Create subfolder for this loop\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(f\"./results/{TRAINING_PREFIX}\"):\n",
    "    shutil.rmtree(f\"./results/{TRAINING_PREFIX}\")\n",
    "os.makedirs(f\"./results/{TRAINING_PREFIX}\")\n",
    "\n",
    "#optim = torch.optim.Adam(ann.parameters(), lr=LR)\n",
    "optim = torch.optim.SGD(ann.parameters(), lr=LR)\n",
    "\n",
    "last_loss = 0\n",
    "last_eval_loss = 0\n",
    "\n",
    "epoch_loss_list = []\n",
    "eval_loss_list = [] \n",
    "\n",
    "fig = create_loss_plot()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        clear_output(wait=True)\n",
    "        fig = create_loss_plot()\n",
    "        update_loss_plot(fig, epoch_loss_list, eval_loss_list)\n",
    "    \n",
    "    epoch_loss, train_R2_score = do_epoch(train_loader, ann, optim, device, n_epochs=N_EPOCHS, current_epoch=epoch, train=True)\n",
    "    eval_epoch_loss, eval_R2_score = do_epoch(test_loader, ann, None, device, n_epochs=N_EPOCHS, current_epoch=epoch, train=False)\n",
    "    eval_loss_list.append(eval_epoch_loss/len(test_loader))\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss/len(train_loader)}, Diff: {epoch_loss/len(train_loader) - last_loss}, Eval Loss: {eval_epoch_loss/len(test_loader)}, Diff Eval: {eval_epoch_loss/len(test_loader) - last_eval_loss}, Train R2 Score: {train_R2_score}, Eval R2 Score: {eval_R2_score}\")\n",
    "    last_loss = epoch_loss/len(train_loader)\n",
    "    last_eval_loss = eval_epoch_loss/len(test_loader)\n",
    "    epoch_loss_list.append(epoch_loss/len(train_loader))\n",
    "    update_loss_plot(fig, epoch_loss_list, eval_loss_list)\n",
    "    if epoch % SAVE_EVERY == 0:\n",
    "        torch.save(ann.state_dict(), f\"./results/{TRAINING_PREFIX}/ann_epoch_{epoch}.pth\")\n",
    "    elif epoch == N_EPOCHS - 1:\n",
    "        torch.save(ann.state_dict(), f\"./results/{TRAINING_PREFIX}/ann_epoch_{epoch}.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(ann.state_dict(), \"largerer_ann_new_data5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e217a28ea0749d98651fdb2e69fb796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'color': 'blue'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Train Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '1f97207a-01f0-4bbb-b7d3-bf540216f47b',\n",
       "              'x': [],\n",
       "              'y': []},\n",
       "             {'line': {'color': 'orange'},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Eval Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd7e2ddbd-79a8-4e4f-8b1c-baa96d494762',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training and Evaluation Losses'},\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = create_loss_plot()\n",
    "update_loss_plot(fig, epoch_loss_list, eval_loss_list, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickled feature scaler\n",
    "import pickle\n",
    "\n",
    "with open(\"extra_lin_around_mean_scaler_low_std_range_very_low_std_feature.pkl\", \"rb\") as f:\n",
    "    feature_scaler = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.19258452,  0.85740802, -0.00171653]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_scaler.transform([[0.5, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.19258452,  0.85740802, -0.00171653]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.5, 0.5, 0.5]])\n",
    "transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8039212730565533)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.eval()\n",
    "transformed = feature_scaler.transform([[0.5, 0.5, 0.5]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7978845608028654)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(4.004183165096531)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.1, 0.1, 0.1]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.989422804014327)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.1, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(29.419972487232553)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.01, 0.01, 0.01]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(39.894228040143275)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.01, 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.44831923645571237)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.9, 0.9, 0.9]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.44326920044603635)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.9, 0.9, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5900993465566204)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.9, 0.7, 0.1]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.53990966513188)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.9, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.3220985685996185)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[0.643, 0.7, 0.1]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.3912431320419234)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(0.643, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-0.0060055248098771635)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[4, 0.3, 0.2]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.57716245835995e-75)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(4, 0.3, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22517786, -0.39839161, -0.88339008,  0.74481909,  1.40303131,\n",
       "       -0.84874733, -1.40303131,  0.08660687,  0.25982061,  0.77946184,\n",
       "        0.6062481 , -0.57160535,  0.15589237,  1.22981757, -1.68017329,\n",
       "        1.19517482, -1.64553055, -1.43767406,  0.67553359,  1.36838856,\n",
       "        0.4676771 , -0.74481909, -1.22981757, -1.6108878 , -0.91803283,\n",
       "        1.29910306, -0.29446336, -0.67553359, -0.64089084, -0.77946184,\n",
       "        0.64089084, -0.36374886,  0.32910611, -0.15589237, -1.05660382,\n",
       "        0.91803283,  1.50695955, -1.50695955,  0.81410459, -1.4723168 ,\n",
       "       -0.22517786, -1.16053207, -0.05196412, -0.19053512,  1.26446031,\n",
       "       -1.09124657, -0.01732137, -1.36838856,  1.12588932,  1.64553055,\n",
       "       -1.26446031,  1.71481604, -1.02196108,  0.50231985,  0.84874733,\n",
       "        0.29446336,  0.43303435,  0.01732137, -0.50231985,  0.05196412,\n",
       "        1.5416023 ,  0.95267558,  1.16053207, -1.33374581, -0.5369626 ,\n",
       "        0.57160535,  0.88339008,  1.33374581,  0.71017634,  0.12124962,\n",
       "       -0.25982061,  0.98731833, -0.98731833,  1.05660382,  1.09124657,\n",
       "       -1.71481604, -1.12588932, -0.95267558,  1.57624505, -0.6062481 ,\n",
       "        1.43767406,  1.4723168 ,  1.68017329,  0.19053512,  1.6108878 ,\n",
       "        0.5369626 , -1.57624505,  1.02196108, -0.32910611, -0.43303435,\n",
       "       -1.19517482, -0.4676771 ,  0.36374886, -0.81410459, -1.5416023 ,\n",
       "       -1.29910306, -0.12124962, -0.71017634,  0.39839161, -0.08660687])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.mean.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64089084, -0.91803283, -1.19517482,  1.4723168 ,  0.25982061,\n",
       "        1.64553055, -1.71481604,  1.50695955, -1.57624505, -1.6108878 ,\n",
       "        1.68017329,  0.15589237, -1.40303131, -0.25982061,  0.22517786,\n",
       "       -0.5369626 , -1.22981757, -0.81410459, -1.68017329,  0.19053512,\n",
       "        0.74481909,  1.09124657,  0.32910611, -0.15589237, -0.08660687,\n",
       "       -1.33374581,  0.36374886,  0.39839161, -0.22517786,  0.67553359,\n",
       "        0.88339008, -0.12124962,  1.12588932, -0.6062481 , -1.16053207,\n",
       "        1.43767406, -1.09124657, -0.71017634,  0.4676771 ,  0.91803283,\n",
       "        1.6108878 ,  0.29446336,  0.84874733,  1.26446031, -0.4676771 ,\n",
       "        1.40303131,  1.33374581, -1.29910306, -0.77946184,  0.81410459,\n",
       "        0.43303435,  0.5369626 , -0.01732137,  1.29910306, -1.36838856,\n",
       "        0.95267558,  1.36838856, -1.12588932, -1.02196108, -0.05196412,\n",
       "       -1.43767406,  1.5416023 , -0.88339008,  0.01732137, -0.29446336,\n",
       "        0.57160535, -0.64089084, -0.36374886, -0.84874733, -0.43303435,\n",
       "       -1.5416023 ,  0.05196412, -0.98731833, -0.57160535,  1.71481604,\n",
       "        1.16053207, -0.50231985, -1.64553055, -0.67553359, -0.95267558,\n",
       "        1.19517482, -0.39839161,  1.02196108,  0.08660687,  1.05660382,\n",
       "       -0.19053512,  0.50231985, -0.74481909,  0.6062481 ,  0.98731833,\n",
       "       -1.4723168 ,  1.57624505, -1.05660382,  0.71017634, -1.26446031,\n",
       "       -1.50695955, -0.32910611,  0.77946184,  1.22981757,  0.12124962])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.std.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.23507488334068374)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# std 1.25 and mean -3.04\n",
    "transformed = feature_scaler.transform([[-2, -3.04, 1.25]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.22578002723581)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(-2, -3.04, 1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lutz\\Bachelorarbeit2\\SNN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(27.346752195786582)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = feature_scaler.transform([[-3.04, -3.04, 0.001]])\n",
    "tensor = torch.tensor(transformed).to(device).float()\n",
    "np.expm1(ann(tensor).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.694598626688652e-20)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_probability(-3.03, -3.04, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
